# DistilBERT model configuration for Yelp sentiment analysis with hyperparameter tuning

# Data configuration
data:
  path: data/yelp_reviews.json
  max_length: 128
  batch_size: 512

# Model architecture - dropout will be tuned
model:
  pretrained_model: distilbert-base-uncased
  dropout: 0.1

# Training configuration - optimizer, learning_rate, and weight_decay will be tuned
training:
  epochs: 100
  optimizer: adamw
  learning_rate: 2e-5
  weight_decay: 0.01
  scheduler: onecycle
  seed: 42

# Hyperparameter tuning
hyperparameter_tuning:
  enabled: true  # Enable hyperparameter tuning
  n_trials: 20   # Number of trials to run
  cv_folds: 3    # Number of cross-validation folds
  n_epochs: 2  # Number of epochs for each trial
  best_params: null  # Will be filled after tuning