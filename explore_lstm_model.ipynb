{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the LSTM Sentiment Analysis Model\n",
    "\n",
    "This notebook allows you to explore the trained LSTM model, load it from a configuration file, and test it with your own text input using an interactive widget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from src.data import YelpDataProcessor\n",
    "from src.models import LSTMSentimentModel\n",
    "import logging\n",
    "\n",
    "# Configure logging to be less verbose in the notebook\n",
    "logging.basicConfig(level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Configuration and Models\n",
    "\n",
    "First, let's load the model configuration and trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which configuration to use\n",
    "config_selector = widgets.Dropdown(\n",
    "    options=[\n",
    "        ('Default LSTM Config', 'model_configs/lstm_default.yaml'),\n",
    "        ('Tuned LSTM Config', 'model_configs/lstm_tuning_v1.yaml')\n",
    "    ],\n",
    "    value='model_configs/lstm_default.yaml',\n",
    "    description='Config:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "display(config_selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path):\n",
    "    \"\"\"Load configuration from YAML file\"\"\"\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "def load_model_and_processor(config_path):\n",
    "    \"\"\"Load model and data processor based on config\"\"\"\n",
    "    # Load configuration\n",
    "    print(f\"Loading configuration from {config_path}\")\n",
    "    config = load_config(config_path)\n",
    "    \n",
    "    # Extract configuration values\n",
    "    data_config = config.get('data', {})\n",
    "    model_config = config.get('model', {})\n",
    "    hp_tuning_config = config.get('hyperparameter_tuning', {})\n",
    "    best_params = hp_tuning_config.get('best_params', None)\n",
    "    \n",
    "    # Apply best parameters if they exist, otherwise use defaults\n",
    "    effective_model_config = model_config.copy()\n",
    "    if best_params is not None:\n",
    "        for param, value in best_params.items():\n",
    "            if param in ['embedding_dim', 'hidden_size', 'dropout', 'bidirectional'] and param in best_params:\n",
    "                effective_model_config[param] = value\n",
    "            if param == 'attention':\n",
    "                effective_model_config['use_attention'] = value\n",
    "                \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize data processor\n",
    "    data_processor = YelpDataProcessor(\n",
    "        data_path=data_config.get('path'),\n",
    "        max_length=data_config.get('max_length', 128),\n",
    "        batch_size=data_config.get('batch_size', 32),\n",
    "        tokenization_method=data_config.get('tokenization_method', 'bpe')\n",
    "    )\n",
    "    \n",
    "    # Load label encoder\n",
    "    label_encoder_path = 'models/label_encoder.pkl'\n",
    "    if os.path.exists(label_encoder_path):\n",
    "        with open(label_encoder_path, 'rb') as f:\n",
    "            data_processor.label_encoder = pickle.load(f)\n",
    "    else:\n",
    "        print(\"Label encoder not found. Loading sample data to create one.\")\n",
    "        df = data_processor.load_data()\n",
    "        data_processor.prepare_data_lstm(df, max_vocab_size=data_config.get('max_vocab_size', 10000))\n",
    "    \n",
    "    # Load tokenizer based on tokenization method\n",
    "    tokenization_method = data_config.get('tokenization_method', 'bpe')\n",
    "    if tokenization_method == 'bpe':\n",
    "        tokenizer_path = 'models/bpe_tokenizer.json'\n",
    "        if os.path.exists(tokenizer_path):\n",
    "            data_processor.load_bpe_tokenizer(tokenizer_path)\n",
    "        else:\n",
    "            print(\"BPE tokenizer not found. Please train the model first.\")\n",
    "            return None, None\n",
    "    else:  # word tokenization\n",
    "        vocab_path = 'models/word_vocab.json'\n",
    "        if os.path.exists(vocab_path):\n",
    "            with open(vocab_path, 'r') as f:\n",
    "                vocab_data = yaml.safe_load(f)\n",
    "                data_processor.word_to_idx = vocab_data.get('word_to_idx', {})\n",
    "                data_processor.idx_to_word = vocab_data.get('idx_to_word', {})\n",
    "                data_processor.vocab_size = vocab_data.get('vocab_size', 0)\n",
    "        else:\n",
    "            print(\"Word vocabulary not found. Please train the model first.\")\n",
    "            return None, None\n",
    "    \n",
    "    # Determine model name/directory\n",
    "    model_name = config.get('name', 'lstm_model')\n",
    "    model_dir = f\"models/{model_name}\"\n",
    "    \n",
    "    # Build model with configuration\n",
    "    model = LSTMSentimentModel(\n",
    "        vocab_size=data_processor.vocab_size,\n",
    "        embedding_dim=effective_model_config.get('embedding_dim', 128),\n",
    "        hidden_size=effective_model_config.get('hidden_size', 64),\n",
    "        num_classes=len(data_processor.label_encoder.classes_),\n",
    "        bidirectional=effective_model_config.get('bidirectional', True),\n",
    "        dropout=effective_model_config.get('dropout', 0.2),\n",
    "        use_attention=effective_model_config.get('use_attention', True),\n",
    "        max_length=data_config.get('max_length', 128),\n",
    "        padding_idx=0,\n",
    "        num_layers=effective_model_config.get('num_layers', 1)\n",
    "    )\n",
    "    \n",
    "    # Load model weights if available\n",
    "    best_model_path = os.path.join(model_dir, 'model_best_f1.pt')\n",
    "    if os.path.exists(best_model_path):\n",
    "        model.load_state_dict(torch.load(best_model_path, map_location=device, weights_only=True))\n",
    "        print(f\"Loaded trained model from {best_model_path}\")\n",
    "    else:\n",
    "        print(f\"Trained model not found at {best_model_path}. Using untrained model.\")\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    return model, data_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and processor based on selected config\n",
    "model, data_processor = load_model_and_processor(config_selector.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Architecture and Configuration\n",
    "\n",
    "Let's examine our model architecture and the configuration that was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model architecture\n",
    "print(\"LSTM Sentiment Analysis Model Architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Display configuration details\n",
    "config = load_config(config_selector.value)\n",
    "print(\"\\nConfiguration:\")\n",
    "for section, params in config.items():\n",
    "    print(f\"\\n{section.upper()}:\")\n",
    "    if isinstance(params, dict):\n",
    "        for param, value in params.items():\n",
    "            print(f\"  {param}: {value}\")\n",
    "    else:\n",
    "        print(f\"  {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Interactive Text Classification\n",
    "\n",
    "Now let's create a widget to input your own text and see the sentiment prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, data_processor, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\"Process text and make a prediction\"\"\"\n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess the text\n",
    "    processed_text = data_processor.preprocess_text(text)\n",
    "    \n",
    "    # Convert to sequence based on tokenization method\n",
    "    if data_processor.tokenization_method == 'bpe':\n",
    "        sequence = data_processor.texts_to_sequences_bpe([processed_text])[0]\n",
    "    else:\n",
    "        sequence = data_processor.texts_to_sequences_word([processed_text])[0]\n",
    "    \n",
    "    # Pad sequence\n",
    "    padded_sequence = data_processor.pad_sequences([sequence], maxlen=data_processor.max_length)[0]\n",
    "    \n",
    "    # Convert to tensor\n",
    "    input_tensor = torch.tensor(padded_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(output, dim=1)[0]\n",
    "        predicted_class = torch.argmax(probabilities).item()\n",
    "    \n",
    "    # Get class name and probabilities\n",
    "    predicted_label = data_processor.label_encoder.classes_[predicted_class]\n",
    "    probs_dict = {data_processor.label_encoder.classes_[i]: prob.item() for i, prob in enumerate(probabilities)}\n",
    "    \n",
    "    return predicted_label, probs_dict, processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create UI elements\n",
    "text_input = widgets.Textarea(\n",
    "    value='This restaurant was amazing! The food was delicious and the service was excellent.',\n",
    "    placeholder='Enter your text here...',\n",
    "    description='Review:',\n",
    "    layout=widgets.Layout(width='100%', height='100px')\n",
    ")\n",
    "\n",
    "run_button = widgets.Button(\n",
    "    description='Analyze Sentiment',\n",
    "    button_style='primary',\n",
    "    tooltip='Click to analyze the sentiment of the text'\n",
    ")\n",
    "\n",
    "config_change_button = widgets.Button(\n",
    "    description='Change Config',\n",
    "    button_style='info',\n",
    "    tooltip='Click to load model from the selected config'\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# Text styling for output\n",
    "def style_prediction(label, probabilities):\n",
    "    \"\"\"Style the prediction output with colors and bars\"\"\"\n",
    "    colors = {\n",
    "        'positive': 'green',\n",
    "        'neutral': 'orange',\n",
    "        'negative': 'red'\n",
    "    }\n",
    "    \n",
    "    result = f\"<h3>Prediction: <span style='color:{colors.get(label, 'blue')}'>{label.upper()}</span></h3>\"\n",
    "    result += \"<h4>Confidence Scores:</h4>\"\n",
    "    \n",
    "    for label, prob in sorted(probabilities.items(), key=lambda x: x[1], reverse=True):\n",
    "        percentage = prob * 100\n",
    "        color = colors.get(label, 'blue')\n",
    "        result += f\"<div style='margin-bottom:5px;'>\"\n",
    "        result += f\"<span style='display:inline-block; width:100px;'>{label}:</span>\"\n",
    "        result += f\"<div style='display:inline-block; width:{percentage}%; background-color:{color}; height:20px;'></div>\"\n",
    "        result += f\"<span style='margin-left:10px;'>{percentage:.2f}%</span>\"\n",
    "        result += \"</div>\"\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Define button click handlers\n",
    "def on_run_button_clicked(b):\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        if model is None or data_processor is None:\n",
    "            print(\"Error: Model or data processor not loaded properly.\")\n",
    "            return\n",
    "        \n",
    "        text = text_input.value\n",
    "        if not text.strip():\n",
    "            print(\"Please enter some text to analyze.\")\n",
    "            return\n",
    "        \n",
    "        predicted_label, probabilities, processed_text = predict_sentiment(\n",
    "            text, model, data_processor\n",
    "        )\n",
    "        \n",
    "        print(f\"Original text: {text}\")\n",
    "        print(f\"Processed text: {processed_text}\")\n",
    "        display(HTML(style_prediction(predicted_label, probabilities)))\n",
    "\n",
    "def on_config_change_clicked(b):\n",
    "    global model, data_processor\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        try:\n",
    "            model, data_processor = load_model_and_processor(config_selector.value)\n",
    "            print(\"Model and data processor loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "\n",
    "# Attach click handlers\n",
    "run_button.on_click(on_run_button_clicked)\n",
    "config_change_button.on_click(on_config_change_clicked)\n",
    "\n",
    "# Display UI\n",
    "display(text_input)\n",
    "display(widgets.HBox([run_button, config_change_button]))\n",
    "display(output_area)\n",
    "\n",
    "# Initialize prediction\n",
    "on_run_button_clicked(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploring Model Predictions on Sample Reviews\n",
    "\n",
    "Let's look at some sample reviews and how the model predicts them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_reviews = [\n",
    "    \"The food was absolutely terrible. I'll never come back to this restaurant again.\",\n",
    "    \"The service was okay, but the food was mediocre. Not worth the price.\",\n",
    "    \"It was an average experience. Nothing special but not bad either.\",\n",
    "    \"The staff was friendly and the atmosphere was nice, but the food was just decent.\",\n",
    "    \"Amazing experience! The chef prepared the best meal I've had in years.\"\n",
    "]\n",
    "\n",
    "for i, review in enumerate(sample_reviews):\n",
    "    print(f\"\\nSample {i+1}: {review}\")\n",
    "    predicted_label, probabilities, processed_text = predict_sentiment(\n",
    "        review, model, data_processor\n",
    "    )\n",
    "    print(f\"Prediction: {predicted_label}\")\n",
    "    print(\"Probabilities:\")\n",
    "    for label, prob in sorted(probabilities.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {label}: {prob:.4f} ({prob*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exploring Token Attention (if model uses attention)\n",
    "\n",
    "If the model uses attention, we can visualize which words the model pays attention to when making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_weights(text, model, data_processor, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\"Get attention weights from the model\"\"\"\n",
    "    # Make sure model uses attention\n",
    "    if not model.use_attention:\n",
    "        print(\"This model doesn't use attention mechanism.\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Preprocess the text\n",
    "    processed_text = data_processor.preprocess_text(text)\n",
    "    \n",
    "    # Convert to tokens for visualization\n",
    "    if data_processor.tokenization_method == 'bpe':\n",
    "        tokens = data_processor.lstm_tokenizer.encode(processed_text).tokens\n",
    "        sequence = data_processor.texts_to_sequences_bpe([processed_text])[0]\n",
    "    else:\n",
    "        tokens = processed_text.split()\n",
    "        sequence = data_processor.texts_to_sequences_word([processed_text])[0]\n",
    "    \n",
    "    # Pad sequence\n",
    "    padded_sequence = data_processor.pad_sequences([sequence], maxlen=data_processor.max_length)[0]\n",
    "    \n",
    "    # Convert to tensor\n",
    "    input_tensor = torch.tensor(padded_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Register a hook to get attention weights\n",
    "    attention_weights = []\n",
    "    def hook_fn(module, input, output):\n",
    "        attention_weights.append(output[1].detach().cpu().numpy())\n",
    "    \n",
    "    if hasattr(model, 'attention'):\n",
    "        hook = model.attention.register_forward_hook(hook_fn)\n",
    "    \n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            probabilities = torch.nn.functional.softmax(output, dim=1)[0]\n",
    "            predicted_class = torch.argmax(probabilities).item()\n",
    "        \n",
    "        # Remove the hook\n",
    "        hook.remove()\n",
    "        \n",
    "        # Get class name and probabilities\n",
    "        predicted_label = data_processor.label_encoder.classes_[predicted_class]\n",
    "        \n",
    "        # Limit tokens to the actual text length (remove padding)\n",
    "        valid_token_length = min(len(tokens), data_processor.max_length)\n",
    "        tokens = tokens[:valid_token_length]\n",
    "        weights = attention_weights[0][0][:valid_token_length]\n",
    "        \n",
    "        return tokens, weights, predicted_label\n",
    "    else:\n",
    "        print(\"This model doesn't have the expected attention structure.\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison between BPE and Word Tokenization\n",
    "\n",
    "If you have models trained with both tokenization methods, you can compare their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to visualize attention weights\n",
    "def visualize_attention(text):\n",
    "    tokens, weights, predicted_label = get_attention_weights(text, model, data_processor)\n",
    "    \n",
    "    if tokens is None or weights is None:\n",
    "        print(\"Couldn't extract attention weights.\")\n",
    "        return\n",
    "    \n",
    "    # Normalize weights for visualization\n",
    "    max_weight = max(weights)\n",
    "    norm_weights = [w / max_weight for w in weights]\n",
    "    \n",
    "    # Create HTML visualization\n",
    "    html = f\"<h3>Attention Visualization for: <span style='color:blue'>{predicted_label.upper()}</span></h3>\"\n",
    "    html += \"<div style='line-height: 2.5; font-family: monospace; font-size: 16px;'>\"\n",
    "    \n",
    "    for token, weight in zip(tokens, norm_weights):\n",
    "        # Map weight to color intensity\n",
    "        color_intensity = int( 255 * ( 1 - weight ) )\n",
    "        background_color = f\"rgb(255, {color_intensity}, {color_intensity})\"\n",
    "        \n",
    "        html += f\"<span style='background-color: {background_color}; padding: 3px; margin: 2px; border-radius: 3px;'>{token}</span>\"\n",
    "    \n",
    "    html += \"</div>\"\n",
    "    display(HTML(html))\n",
    "\n",
    "# Create UI for attention visualization\n",
    "attention_text = widgets.Textarea(\n",
    "    value='The food was delicious but the service was terrible.',\n",
    "    placeholder='Enter text to visualize attention...',\n",
    "    description='Text:',\n",
    "    layout=widgets.Layout(width='100%', height='100px')\n",
    ")\n",
    "\n",
    "attention_button = widgets.Button(\n",
    "    description='Visualize Attention',\n",
    "    button_style='success',\n",
    "    tooltip='Click to visualize token attention'\n",
    ")\n",
    "\n",
    "attention_output = widgets.Output()\n",
    "\n",
    "def on_attention_button_clicked(b):\n",
    "    with attention_output:\n",
    "        attention_output.clear_output()\n",
    "        visualize_attention(attention_text.value)\n",
    "\n",
    "attention_button.on_click(on_attention_button_clicked)\n",
    "\n",
    "# Display attention UI\n",
    "print(\"\\nVisualize which words the model pays attention to:\")\n",
    "display(attention_text)\n",
    "display(attention_button)\n",
    "display(attention_output)\n",
    "\n",
    "# Initialize visualization\n",
    "on_attention_button_clicked(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
